{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3b53bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [18/Mar/2024 18:14:17] \"GET / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FileStorage: 'ezgif-frame-002_jpg-Copy1.rf.e39acbb2171865635d4641ed0856432a.jpg' ('image/jpeg')>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-03-18 18:14:29,400] ERROR in app: Exception on /predict [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mcheruku\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 2073, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"C:\\Users\\mcheruku\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1519, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"C:\\Users\\mcheruku\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1517, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"C:\\Users\\mcheruku\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1503, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**req.view_args)\n",
      "  File \"C:\\Users\\mcheruku\\AppData\\Local\\Temp\\ipykernel_18264\\1407478682.py\", line 95, in upload\n",
      "    preds = model_predict(f, model)\n",
      "  File \"C:\\Users\\mcheruku\\AppData\\Local\\Temp\\ipykernel_18264\\1407478682.py\", line 60, in model_predict\n",
      "    preds = model.predict(file)\n",
      "  File \"C:\\Users\\mcheruku\\Anaconda3\\lib\\site-packages\\ultralytics\\engine\\model.py\", line 439, in predict\n",
      "    return self.predictor.predict_cli(source=source) if is_cli else self.predictor(source=source, stream=stream)\n",
      "  File \"C:\\Users\\mcheruku\\Anaconda3\\lib\\site-packages\\ultralytics\\engine\\predictor.py\", line 168, in __call__\n",
      "    return list(self.stream_inference(source, model, *args, **kwargs))  # merge list of Result into one\n",
      "  File \"C:\\Users\\mcheruku\\Anaconda3\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 35, in generator_context\n",
      "    response = gen.send(None)\n",
      "  File \"C:\\Users\\mcheruku\\Anaconda3\\lib\\site-packages\\ultralytics\\engine\\predictor.py\", line 220, in stream_inference\n",
      "    self.setup_source(source if source is not None else self.args.source)\n",
      "  File \"C:\\Users\\mcheruku\\Anaconda3\\lib\\site-packages\\ultralytics\\engine\\predictor.py\", line 192, in setup_source\n",
      "    self.dataset = load_inference_source(\n",
      "  File \"C:\\Users\\mcheruku\\Anaconda3\\lib\\site-packages\\ultralytics\\data\\build.py\", line 166, in load_inference_source\n",
      "    source, stream, screenshot, from_img, in_memory, tensor = check_source(source)\n",
      "  File \"C:\\Users\\mcheruku\\Anaconda3\\lib\\site-packages\\ultralytics\\data\\build.py\", line 148, in check_source\n",
      "    raise TypeError(\"Unsupported image type. For supported types see https://docs.ultralytics.com/modes/predict\")\n",
      "TypeError: Unsupported image type. For supported types see https://docs.ultralytics.com/modes/predict\n",
      "127.0.0.1 - - [18/Mar/2024 18:14:29] \"POST /predict HTTP/1.1\" 500 -\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division, print_function\n",
    "# coding=utf-8\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.2\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "# Keras\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Flask utils\n",
    "from flask import Flask, redirect, url_for, request, render_template\n",
    "from werkzeug.utils import secure_filename\n",
    "#from gevent.pywsgi import WSGIServer\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Define a flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Model saved with Keras model.save()\n",
    "MODEL_PATH ='engine/best.pt'\n",
    "\n",
    "# Load your trained model\n",
    "model = YOLO(MODEL_PATH)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def model_predict(file, model):\n",
    "    print(file)\n",
    "    #img = image.load_img(file, target_size=(224, 224))\n",
    "\n",
    "    # Preprocessing the image\n",
    "    #x = image.img_to_array(img)\n",
    "    # x = np.true_divide(x, 255)\n",
    "    ## Scaling\n",
    "    #x=x/255\n",
    "    #x = np.expand_dims(x, axis=0)\n",
    "   \n",
    "\n",
    "    # Be careful how your trained model deals with the input\n",
    "    # otherwise, it won't make correct prediction!\n",
    "    # x = preprocess_input(x)\n",
    "\n",
    "    preds = model.predict(file)\n",
    "    preds=np.argmax(preds, axis=1)\n",
    "    if preds==0:\n",
    "        preds=\"The leaf is diseased cotton leaf\"\n",
    "    elif preds==1:\n",
    "        preds=\"The leaf is diseased cotton plant\"\n",
    "    elif preds==2:\n",
    "        preds=\"The leaf is fresh cotton leaf\"\n",
    "    else:\n",
    "        preds=\"The leaf is fresh cotton plant\"\n",
    "        \n",
    "    \n",
    "    \n",
    "    return preds\n",
    "\n",
    "\n",
    "@app.route('/', methods=['GET'])\n",
    "def index():\n",
    "    # Main page\n",
    "    return render_template('index.html')\n",
    "\n",
    "\n",
    "@app.route('/predict', methods=['GET', 'POST'])\n",
    "def upload():\n",
    "    if request.method == 'POST':\n",
    "        # Get the file from post request\n",
    "        f = request.files['file']\n",
    "\n",
    "        # Save the file to ./uploads\n",
    "        basepath = os.path.dirname(__file__)\n",
    "        file_path = os.path.join(\n",
    "             basepath, 'uploads', secure_filename(f.filename))\n",
    "        f.save(file_path)\n",
    "\n",
    "        # Make prediction\n",
    "        preds = model_predict(file_path, model)\n",
    "    return render_template('index.html',prediction_text=\"type is {}\".format(preds))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7b7cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install Flask==2.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf6c7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install Werkzeug==2.3.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1c910d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\mcheruku\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\mcheruku\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\mcheruku\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\mcheruku\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\mcheruku\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\mcheruku\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\mcheruku\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\mcheruku\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\mcheruku\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\mcheruku\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\mcheruku\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\mcheruku\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\mcheruku\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\mcheruku\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\mcheruku\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\mcheruku\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\mcheruku\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\mcheruku\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install ultralytics -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8157c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\mcheruku\\Downloads\\u - Copy\\u - Copy\\ezgif-frame-002_jpg-Copy1.rf.e39acbb2171865635d4641ed0856432a.jpg: 640x640 1 Oleander, 2261.5ms\n",
      "Speed: 16.1ms preprocess, 2261.5ms inference, 15.7ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: None\n",
       " names: {0: 'Azadiractha Indica', 1: 'Calotropis', 2: 'Ficus Religiosa-Raavi-', 3: 'Oleander'}\n",
       " obb: None\n",
       " orig_img: array([[[ 67,  92,  96],\n",
       "         [ 94, 119, 123],\n",
       "         [ 99, 121, 126],\n",
       "         ...,\n",
       "         [191, 192, 202],\n",
       "         [192, 193, 203],\n",
       "         [192, 193, 203]],\n",
       " \n",
       "        [[ 77, 102, 106],\n",
       "         [ 99, 124, 128],\n",
       "         [111, 133, 138],\n",
       "         ...,\n",
       "         [189, 190, 200],\n",
       "         [192, 193, 203],\n",
       "         [194, 195, 205]],\n",
       " \n",
       "        [[ 89, 116, 120],\n",
       "         [104, 131, 135],\n",
       "         [116, 141, 145],\n",
       "         ...,\n",
       "         [186, 187, 197],\n",
       "         [190, 191, 201],\n",
       "         [193, 194, 204]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[125, 171, 205],\n",
       "         [ 77, 123, 157],\n",
       "         [ 65, 112, 144],\n",
       "         ...,\n",
       "         [115, 162, 200],\n",
       "         [119, 166, 204],\n",
       "         [118, 166, 202]],\n",
       " \n",
       "        [[114, 146, 181],\n",
       "         [ 69, 102, 135],\n",
       "         [ 59,  92, 125],\n",
       "         ...,\n",
       "         [129, 178, 218],\n",
       "         [126, 175, 215],\n",
       "         [125, 174, 212]],\n",
       " \n",
       "        [[ 30,  57,  91],\n",
       "         [ 15,  42,  76],\n",
       "         [ 24,  52,  83],\n",
       "         ...,\n",
       "         [139, 189, 231],\n",
       "         [135, 184, 224],\n",
       "         [136, 185, 225]]], dtype=uint8)\n",
       " orig_shape: (640, 640)\n",
       " path: 'C:\\\\Users\\\\mcheruku\\\\Downloads\\\\u - Copy\\\\u - Copy\\\\ezgif-frame-002_jpg-Copy1.rf.e39acbb2171865635d4641ed0856432a.jpg'\n",
       " probs: None\n",
       " save_dir: 'runs\\\\detect\\\\predict'\n",
       " speed: {'preprocess': 16.126155853271484, 'inference': 2261.4660263061523, 'postprocess': 15.65408706665039}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict('ezgif-frame-002_jpg-Copy1.rf.e39acbb2171865635d4641ed0856432a.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0c7119",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
